# Scaling Laws & Training Resources

## Training Libraries
- **DeepSpeed** – Efficient large-scale training library: https://www.deepspeed.ai/training/
- **Megatron-LM** – Framework for training huge transformer models: https://github.com/NVIDIA/Megatron-LM

## Datasets
- **CommonCrawl** – Massive web-crawl text dataset: https://commoncrawl.org/
- **Semantic Scholar** – Scientific papers corpus: https://www.semanticscholar.org/
- **arXiv** – Research papers repository: https://arxiv.org/
- **Bible Dataset** – Structured biblical text: https://www.kaggle.com/datasets/bradystephenson/bibledata
- **Wikipedia** – Large encyclopedic dataset: https://huggingface.co/datasets/wikimedia/wikipedia

## To Read
- **AfroXLMR-Social** – African-language social text model: https://huggingface.co/Tadesse/AfroXLMR-Social
- **Emergent Properties in LLMs** – Article on emergent abilities: https://gregrobison.medium.com/emergent-properties-in-large-language-models-llms-deep-research-81421065d0ce
https://huggingface.co/blog/hf-skills-training
- **Demis Hassabis (DeepMind CEO)** – Founder of DeepMind, 2010: https://deepmind.google/people/DemisHassabis/
- **Learning How to Learn** – Book on effective learning: https://www.goodreads.com/book/show/25942882-learning-how-to-learn
