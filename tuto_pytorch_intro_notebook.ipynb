{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josue-do-it/nlp-llm/blob/main/tuto_pytorch_intro_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNpqRrGQT1s5"
      },
      "source": [
        "# Introduction to PyTorch: From Tensors to Training Neural Networks\n",
        "\n",
        "**Based on Sebastian Raschka's PyTorch Tutorial**\n",
        "\n",
        "This notebook will introduce you to the essential concepts of PyTorch in a hands-on, interactive way. By the end of this tutorial, you'll understand:\n",
        "\n",
        "1. What PyTorch is and why it's popular\n",
        "2. Tensors - the fundamental data structure\n",
        "3. Automatic differentiation (autograd)\n",
        "4. Building neural networks\n",
        "5. Training models with a typical training loop\n",
        "6. Working with GPUs\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1: What is PyTorch?\n",
        "\n",
        "PyTorch is an open-source Python deep learning library that has become the most widely used framework for  and model development since 2019. It offers the perfect balance between:\n",
        "- **Ease of use**: Intuitive, Python-native API\n",
        "- **Flexibility**: Full control for customization\n",
        "- **Performance**: GPU acceleration for fast training\n",
        "\n",
        "### The Three Core Components of PyTorch:\n",
        "\n",
        "1. **Tensor Library**: Like NumPy but with GPU support\n",
        "2. **Automatic Differentiation (Autograd)**: Computes gradients automatically for backpropagation\n",
        "3. **Deep Learning Library**: Pre-built modules, loss functions, and optimizers\n",
        "\n",
        "Let's start by installing and checking PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NI9Q_p_LT1s7",
        "outputId": "dc5cae8f-a78a-445e-8359-e64bddac77b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA (GPU) available: True\n",
            "Number of GPUs: 1\n",
            "Current GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Install PyTorch (uncomment if needed)\n",
        "# !pip install torch\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Check PyTorch version\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Check if GPU is available\n",
        "print(f\"CUDA (GPU) available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSPmv8iWT1s8"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Understanding Tensors\n",
        "\n",
        "**What are tensors?**\n",
        "\n",
        "Tensors are the fundamental data structure in PyTorch - they're multi-dimensional arrays that can store data:\n",
        "\n",
        "- **Scalar (0D tensor)**: Just a number (e.g., 5)\n",
        "- **Vector (1D tensor)**: Array of numbers (e.g., [1, 2, 3])\n",
        "- **Matrix (2D tensor)**: Table of numbers (e.g., [[1, 2], [3, 4]])\n",
        "- **3D+ tensors**: Higher-dimensional arrays\n",
        "\n",
        "### Creating Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rix7rWWHT1s9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c12e7bc-48d9-43e9-e422-eac2ac47c8b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scalar: 1\n",
            "Shape: torch.Size([])\n",
            "\n",
            "Vector: tensor([1, 2, 3])\n",
            "Shape: torch.Size([3])\n",
            "\n",
            "Matrix:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "Shape: torch.Size([2, 3])\n",
            "\n",
            "3D Tensor:\n",
            "tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "Shape: torch.Size([2, 2, 2])\n"
          ]
        }
      ],
      "source": [
        "# Create a scalar (0D tensor)\n",
        "tensor0d = torch.tensor(1)\n",
        "print(f\"Scalar: {tensor0d}\")\n",
        "print(f\"Shape: {tensor0d.shape}\\n\")\n",
        "\n",
        "# Create a vector (1D tensor)\n",
        "tensor1d = torch.tensor([1, 2, 3])\n",
        "print(f\"Vector: {tensor1d}\")\n",
        "print(f\"Shape: {tensor1d.shape}\\n\")\n",
        "\n",
        "# Create a matrix (2D tensor)\n",
        "tensor2d = torch.tensor([[1, 2, 3],\n",
        "                         [4, 5, 6]])\n",
        "print(f\"Matrix:\\n{tensor2d}\")\n",
        "print(f\"Shape: {tensor2d.shape}\\n\")\n",
        "\n",
        "# Create a 3D tensor\n",
        "tensor3d = torch.tensor([[[1, 2], [3, 4]],\n",
        "                         [[5, 6], [7, 8]]])\n",
        "print(f\"3D Tensor:\\n{tensor3d}\")\n",
        "print(f\"Shape: {tensor3d.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qg4UlKwT1s9"
      },
      "source": [
        "### Exercise 1: Create Your Own Tensors\n",
        "\n",
        "**TODO**: Create the following tensors:\n",
        "1. A scalar with value 42\n",
        "2. A 1D tensor with values [10, 20, 30, 40, 50]\n",
        "3. A 2Ã—2 identity matrix [[1, 0], [0, 1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hu6UgTOET1s9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "148c85c4-b635-483c-affe-617f91edb0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My scalar: 42\n",
            "My vector: tensor([10, 20, 30, 40, 50])\n",
            "My matrix:\n",
            "tensor([[1, 0],\n",
            "        [0, 1]])\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "my_scalar =  torch.tensor(42)\n",
        "my_vector = torch.tensor([10,20, 30, 40,50])\n",
        "my_matrix = torch.tensor([[1, 0],\n",
        "                          [0, 1]])\n",
        "\n",
        "print(f\"My scalar: {my_scalar}\")\n",
        "print(f\"My vector: {my_vector}\")\n",
        "print(f\"My matrix:\\n{my_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N3dcVgNT1s-"
      },
      "source": [
        "### Tensor Data Types\n",
        "\n",
        "PyTorch tensors have data types (dtypes) that specify how the data is stored:\n",
        "- **Integers**: `torch.int64` (default for Python integers)\n",
        "- **Floats**: `torch.float32` (default for Python floats, most common in deep learning)\n",
        "\n",
        "**Why float32?** It balances precision and computational efficiency, and GPUs are optimized for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J9S3e0LjT1s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42321b74-1942-4d2d-c8ac-7c6b3a35bc02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integer tensor dtype: torch.int64\n",
            "Float tensor dtype: torch.float32\n",
            "Converted dtype: torch.float32\n"
          ]
        }
      ],
      "source": [
        "# Check data types\n",
        "int_tensor = torch.tensor([1, 2, 3])\n",
        "print(f\"Integer tensor dtype: {int_tensor.dtype}\")\n",
        "\n",
        "float_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(f\"Float tensor dtype: {float_tensor.dtype}\")\n",
        "\n",
        "# Convert data types\n",
        "converted = int_tensor.to(torch.float32)\n",
        "print(f\"Converted dtype: {converted.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo4tVGODT1s_"
      },
      "source": [
        "### Common Tensor Operations\n",
        "\n",
        "PyTorch provides a NumPy-like API for tensor operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W_KIVszRT1tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ccdc749-4d83-4f5b-d112-a06b4d52fd9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tensor:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "Shape: torch.Size([2, 3])\n",
            "\n",
            "Reshaped (3Ã—2):\n",
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6]])\n",
            "\n",
            "Transposed:\n",
            "tensor([[1, 4],\n",
            "        [2, 5],\n",
            "        [3, 6]])\n",
            "\n",
            "Matrix multiplication (tensor @ tensor.T):\n",
            "tensor([[14, 32],\n",
            "        [32, 77]])\n"
          ]
        }
      ],
      "source": [
        "# Create a 2D tensor\n",
        "tensor = torch.tensor([[1, 2, 3],\n",
        "                       [4, 5, 6]])\n",
        "\n",
        "print(f\"Original tensor:\\n{tensor}\")\n",
        "print(f\"Shape: {tensor.shape}\\n\")\n",
        "\n",
        "# Reshape/View - change dimensions\n",
        "reshaped = tensor.view(3, 2)\n",
        "print(f\"Reshaped (3Ã—2):\\n{reshaped}\\n\")\n",
        "\n",
        "# Transpose - flip across diagonal\n",
        "transposed = tensor.T\n",
        "print(f\"Transposed:\\n{transposed}\\n\")\n",
        "\n",
        "# Matrix multiplication\n",
        "result = tensor @ tensor.T\n",
        "print(f\"Matrix multiplication (tensor @ tensor.T):\\n{result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVDzw_6qT1tA"
      },
      "source": [
        "### Exercise 2: Tensor Operations\n",
        "\n",
        "**TODO**: Given the tensor below:\n",
        "1. Reshape it to shape (4, 3)\n",
        "2. Compute the transpose\n",
        "3. Find the sum of all elements using `.sum()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IArIZ-HxT1tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47c47f06-39cd-4ddd-a0fc-bf9a8e380ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reshaped:\n",
            "tensor([[ 1,  2,  3],\n",
            "        [ 4,  5,  6],\n",
            "        [ 7,  8,  9],\n",
            "        [10, 11, 12]])\n",
            "\n",
            "Transposed:\n",
            "tensor([[ 1,  5,  9],\n",
            "        [ 2,  6, 10],\n",
            "        [ 3,  7, 11],\n",
            "        [ 4,  8, 12]])\n",
            "\n",
            "Sum of all elements: 78\n"
          ]
        }
      ],
      "source": [
        "practice_tensor = torch.tensor([[1, 2, 3, 4],\n",
        "                                [5, 6, 7, 8],\n",
        "                                [9, 10, 11, 12]])\n",
        "\n",
        "# Your code here\n",
        "reshaped = practice_tensor.view(4, 3)\n",
        "transposed = practice_tensor.T\n",
        "total_sum = practice_tensor.sum()\n",
        "\n",
        "print(f\"Reshaped:\\n{reshaped}\\n\")\n",
        "print(f\"Transposed:\\n{transposed}\\n\")\n",
        "print(f\"Sum of all elements: {total_sum}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5Py4YFET1tB"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Computation Graphs and Automatic Differentiation\n",
        "\n",
        "### What is a Computation Graph?\n",
        "\n",
        "A computation graph tracks the sequence of operations needed to compute an output. PyTorch builds this graph automatically to compute gradients during backpropagation.\n",
        "\n",
        "**Example**: Simple logistic regression forward pass\n",
        "```\n",
        "z = x1 * w1 + b     (net input)\n",
        "a = sigmoid(z)      (activation)\n",
        "loss = BCE(a, y)    (binary cross-entropy loss)\n",
        "```\n",
        "\n",
        "### Automatic Differentiation (Autograd)\n",
        "\n",
        "PyTorch's autograd engine automatically computes gradients - no manual calculus needed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IxkiaOQuT1tB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc884d9-b90a-4739-8868-ffa401c55f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.0852\n",
            "Gradient w.r.t. w1: tensor([-0.0898])\n",
            "Gradient w.r.t. b: tensor([-0.0817])\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Set up a simple computation\n",
        "y = torch.tensor([1.0])  # true label\n",
        "x1 = torch.tensor([1.1])  # input feature\n",
        "\n",
        "# Parameters - requires_grad=True enables gradient tracking\n",
        "w1 = torch.tensor([2.2], requires_grad=True)\n",
        "b = torch.tensor([0.0], requires_grad=True)\n",
        "\n",
        "# Forward pass\n",
        "z = x1 * w1 + b\n",
        "a = torch.sigmoid(z)\n",
        "loss = F.binary_cross_entropy(a, y)\n",
        "\n",
        "print(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Backward pass - compute gradients automatically!\n",
        "loss.backward()\n",
        "\n",
        "print(f\"Gradient w.r.t. w1: {w1.grad}\")\n",
        "print(f\"Gradient w.r.t. b: {b.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qpGvP8eT1tB"
      },
      "source": [
        "**Key Points**:\n",
        "- `requires_grad=True`: Tells PyTorch to track operations for gradient computation\n",
        "- `.backward()`: Computes all gradients automatically\n",
        "- `.grad`: Stores the computed gradient for each tensor\n",
        "\n",
        "This is the magic that makes training neural networks easy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVjDOudrT1tB"
      },
      "source": [
        "### Exercise 3: Manual Gradient Computation\n",
        "\n",
        "**TODO**: Complete the forward pass and compute gradients\n",
        "1. Compute `z = x * w + b`\n",
        "2. Compute the loss: `loss = (z - target)**2` (mean squared error)\n",
        "3. Call `.backward()` to compute gradients\n",
        "4. Print the gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yPLWzoD2T1tB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822301d4-7ee8-4956-dbe7-2d38bc061951"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 9.0000\n",
            "Gradient w.r.t. w: tensor([-12.])\n",
            "Gradient w.r.t. b: tensor([-6.])\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "x = torch.tensor([2.0])\n",
        "target = torch.tensor([5.0])\n",
        "w = torch.tensor([1.0], requires_grad=True)\n",
        "b = torch.tensor([0.0], requires_grad=True)\n",
        "\n",
        "# Your code here\n",
        "z =  x * w + b\n",
        "loss =  (z - target)**2\n",
        "\n",
        "\n",
        "# TODO: call backward to compute gradients\n",
        "loss.backward()\n",
        "\n",
        "print(f\"Loss: {loss.item():.4f}\")\n",
        "print(f\"Gradient w.r.t. w: {w.grad}\")\n",
        "print(f\"Gradient w.r.t. b: {b.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5pfUT6T1tC"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Building Neural Networks\n",
        "\n",
        "In PyTorch, we build neural networks by subclassing `torch.nn.Module`. This gives us:\n",
        "- Automatic parameter tracking\n",
        "- Easy layer composition\n",
        "- Built-in training/evaluation modes\n",
        "\n",
        "### Anatomy of a PyTorch Model\n",
        "\n",
        "```python\n",
        "class MyModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define layers here\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Define forward pass here\n",
        "        return output\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MN3g0LfCT1tC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "065553a8-7530-4c0a-d501-43ade9f7d279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Total trainable parameters: 2213\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            # First hidden layer\n",
        "            nn.Linear(num_inputs, 30),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Second hidden layer\n",
        "            nn.Linear(30, 20),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output layer\n",
        "            nn.Linear(20, num_outputs)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Create a model\n",
        "model = NeuralNetwork(num_inputs=50, num_outputs=3)\n",
        "print(model)\n",
        "\n",
        "# Count parameters\n",
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal trainable parameters: {num_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjVuS3hIT1tC"
      },
      "source": [
        "### Making Predictions\n",
        "\n",
        "To use the model, we pass data through it. The model returns **logits** (raw scores), which we can convert to probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XZVRi6bXT1tC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b10384-8f72-426d-915e-3e606f5daa2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits: tensor([[-0.0782, -0.0795,  0.0759]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Probabilities: tensor([[0.3159, 0.3155, 0.3686]], grad_fn=<SoftmaxBackward0>)\n",
            "Sum of probabilities: 1.0000\n",
            "Predicted class: 2\n"
          ]
        }
      ],
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Create random input (1 sample, 50 features)\n",
        "X = torch.rand((1, 50))\n",
        "\n",
        "# Forward pass - get logits\n",
        "logits = model(X)\n",
        "print(f\"Logits: {logits}\\n\")\n",
        "\n",
        "# Convert to probabilities with softmax\n",
        "probabilities = torch.softmax(logits, dim=1)\n",
        "print(f\"Probabilities: {probabilities}\")\n",
        "print(f\"Sum of probabilities: {probabilities.sum():.4f}\")\n",
        "\n",
        "# Get predicted class\n",
        "predicted_class = torch.argmax(probabilities, dim=1)\n",
        "print(f\"Predicted class: {predicted_class.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZFxY4myT1tD"
      },
      "source": [
        "### Exercise 4: Build Your Own Network\n",
        "\n",
        "**TODO**: Create a neural network with:\n",
        "- Input size: 10\n",
        "- Hidden layer 1: 64 neurons with ReLU activation\n",
        "- Hidden layer 2: 32 neurons with ReLU activation  \n",
        "- Output size: 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ClaFoSfXT1tD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799d51c5-5798-4fc8-b4e1-afaa2d3ce347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyNetwork(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=10, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=5, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Output shape: torch.Size([1, 5])\n"
          ]
        }
      ],
      "source": [
        "class MyNetwork(nn.Module):\n",
        "    def __init__(self, num_input, num_output):\n",
        "        super().__init__()\n",
        "        # TODO: Define your layers using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "            # Your layers here\n",
        "            # First hidden layer\n",
        "            nn.Linear(num_input, 64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Second hidden layer\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            # Output\n",
        "            nn.Linear(32, num_output)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Create and test your model\n",
        "my_model = MyNetwork(num_input=10, num_output=5)\n",
        "print(my_model)\n",
        "\n",
        "# Test with random input\n",
        "test_input = torch.rand((1, 10))\n",
        "output = my_model(test_input)\n",
        "print(f\"\\nOutput shape: {output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_yb5cZDT1tD"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 5: Data Loading\n",
        "\n",
        "PyTorch provides `Dataset` and `DataLoader` classes for efficient data loading:\n",
        "\n",
        "1. **Dataset**: Defines how to access individual samples\n",
        "2. **DataLoader**: Batches data, shuffles, and loads in parallel\n",
        "\n",
        "### Creating a Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qfC_ds6GT1tG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa594f05-c1c2-437b-bc34-ae1a35512750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 5\n",
            "Test samples: 2\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Return one sample\n",
        "        return self.features[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return dataset size\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create toy data\n",
        "X_train = torch.tensor([[-1.2, 3.1],\n",
        "                        [-0.9, 2.9],\n",
        "                        [-0.5, 2.6],\n",
        "                        [2.3, -1.1],\n",
        "                        [2.7, -1.5]])\n",
        "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
        "\n",
        "X_test = torch.tensor([[-0.8, 2.8],\n",
        "                       [2.6, -1.6]])\n",
        "y_test = torch.tensor([0, 1])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ToyDataset(X_train, y_train)\n",
        "test_dataset = ToyDataset(X_test, y_test)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B43f3TzMT1tH"
      },
      "source": [
        "### Creating DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "hw-BsWdST1tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "566c8d7e-f763-4ec2-97ce-b500f455dcd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training batches:\n",
            "Batch 1: Features shape=torch.Size([2, 2]), Labels=tensor([1, 0])\n",
            "Batch 2: Features shape=torch.Size([2, 2]), Labels=tensor([0, 0])\n"
          ]
        }
      ],
      "source": [
        "# Set seed for reproducible shuffling\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    drop_last=True  # Drop last incomplete batch\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Iterate over batches\n",
        "print(\"Training batches:\")\n",
        "for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "    print(f\"Batch {batch_idx + 1}: Features shape={features.shape}, Labels={labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhOFi48BT1tH"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: The Training Loop\n",
        "\n",
        "Now we combine everything: model, data, loss function, and optimizer.\n",
        "\n",
        "### Key Components:\n",
        "1. **Model**: The neural network\n",
        "2. **Loss Function**: Measures prediction error (e.g., cross-entropy)\n",
        "3. **Optimizer**: Updates weights to minimize loss (e.g., SGD, Adam)\n",
        "4. **Training Loop**: Iterate over data, compute loss, update weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FD0fxYSmT1tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a2a510d-eef1-4261-80ac-5485596a8581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 | Batch 1/2 | Loss: 0.7487\n",
            "Epoch 1/3 | Batch 2/2 | Loss: 0.6450\n",
            "Epoch 2/3 | Batch 1/2 | Loss: 0.4423\n",
            "Epoch 2/3 | Batch 2/2 | Loss: 0.1256\n",
            "Epoch 3/3 | Batch 1/2 | Loss: 0.0269\n",
            "Epoch 3/3 | Batch 2/2 | Loss: 0.0043\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Create model (2 inputs, 2 outputs for binary classification)\n",
        "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "\n",
        "# Define optimizer (Stochastic Gradient Descent)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "        # Forward pass\n",
        "        logits = model(features)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        loss.backward()  # Compute gradients\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Logging\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
        "              f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUeEN5xhT1tH"
      },
      "source": [
        "### Model Evaluation\n",
        "\n",
        "After training, we evaluate the model's accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2udG6T4wT1tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1089cb2c-4fcf-4acb-ae0a-393e7623ee50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 100.00%\n",
            "Test Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "def compute_accuracy(model, dataloader):\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation\n",
        "        for features, labels in dataloader:\n",
        "            logits = model(features)\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += len(labels)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "# Evaluate on training and test sets\n",
        "train_acc = compute_accuracy(model, train_loader)\n",
        "test_acc = compute_accuracy(model, test_loader)\n",
        "\n",
        "print(f\"Training Accuracy: {train_acc * 100:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4nPjis-T1tI"
      },
      "source": [
        "### Exercise 5: Complete Training Loop\n",
        "\n",
        "**TODO**: Fill in the missing parts of the training loop below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tgulK1fHT1tI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d42a69-8f74-474f-b59a-4e2956cf0ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Loss: 0.5213\n",
            "Epoch 2/5 | Loss: 0.5350\n",
            "Epoch 3/5 | Loss: 0.4082\n",
            "Epoch 4/5 | Loss: 0.1720\n",
            "Epoch 5/5 | Loss: 0.0916\n",
            "\n",
            "Final Test Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Create a fresh model\n",
        "torch.manual_seed(42)\n",
        "student_model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "\n",
        "# TODO: Create an Adam optimizer with learning rate 0.01\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    student_model.train()\n",
        "\n",
        "    for features, labels in train_loader:\n",
        "        # TODO: Forward pass - compute logits\n",
        "        logits = student_model(features)\n",
        "\n",
        "        # TODO: Compute cross-entropy loss\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # TODO: Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO: Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # TODO: Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print loss every epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluate\n",
        "final_acc = compute_accuracy(student_model, test_loader)\n",
        "print(f\"\\nFinal Test Accuracy: {final_acc * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvkoMUrIT1tI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 7: Saving and Loading Models\n",
        "\n",
        "After training, we want to save our model for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qqDjA2ozT1tI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd77b75-db82-4e7a-b5a7-e45ad3b07f1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved!\n",
            "Model loaded!\n",
            "Loaded model test accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Save model\n",
        "torch.save(model.state_dict(), \"my_model.pth\")\n",
        "print(\"Model saved!\")\n",
        "\n",
        "# Load model\n",
        "loaded_model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "loaded_model.load_state_dict(torch.load(\"my_model.pth\", weights_only=True))\n",
        "loaded_model.eval()\n",
        "print(\"Model loaded!\")\n",
        "\n",
        "# Verify it works\n",
        "test_acc = compute_accuracy(loaded_model, test_loader)\n",
        "print(f\"Loaded model test accuracy: {test_acc * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcdh5vf8T1tI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 8: GPU Training (Optional)\n",
        "\n",
        "If you have a GPU available, you can significantly speed up training.\n",
        "\n",
        "### Key Concepts:\n",
        "1. Move model to GPU: `model.to(device)`\n",
        "2. Move data to GPU: `data.to(device)`\n",
        "3. All tensors must be on the same device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DzouYRWXT1tI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8d5a26-1c54-4c65-e6d5-30a8dcfa48c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1/3 | Batch 1/2 | Loss: 0.7487\n",
            "Epoch 1/3 | Batch 2/2 | Loss: 0.6450\n",
            "Epoch 2/3 | Batch 1/2 | Loss: 0.4423\n",
            "Epoch 2/3 | Batch 2/2 | Loss: 0.1256\n",
            "Epoch 3/3 | Batch 1/2 | Loss: 0.0269\n",
            "Epoch 3/3 | Batch 2/2 | Loss: 0.0043\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create model and move to GPU\n",
        "torch.manual_seed(123)\n",
        "gpu_model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "gpu_model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(gpu_model.parameters(), lr=0.5)\n",
        "\n",
        "# Training loop with GPU\n",
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    gpu_model.train()\n",
        "\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "        # Move data to GPU\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = gpu_model(features)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "              f\"Batch {batch_idx+1}/{len(train_loader)} | \"\n",
        "              f\"Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrOfkKu7T1tI"
      },
      "source": [
        "### Exercise 6: GPU Training\n",
        "\n",
        "**TODO**: Modify the accuracy computation function to work with GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K4C_7ju1T1tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba425f6-bf15-410b-a1d1-26a683a1ab39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Model Test Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "def compute_accuracy_gpu(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels in dataloader:\n",
        "            # TODO: Move features and labels to device\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += len(labels)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "# Test your function\n",
        "test_acc = compute_accuracy_gpu(gpu_model, test_loader, device)\n",
        "print(f\"GPU Model Test Accuracy: {test_acc * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oaq7v3u6T1tJ"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 9: Putting It All Together - Real Example\n",
        "\n",
        "Let's create a complete example with a slightly more complex dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "B4Zm9jkBT1tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e04f17-b671-49d4-b5af-74d4d51f56cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 800\n",
            "Test samples: 200\n",
            "Features: 20\n",
            "Classes: 3\n"
          ]
        }
      ],
      "source": [
        "# Create synthetic dataset\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Generate 1000 samples with 20 features\n",
        "n_samples = 1000\n",
        "n_features = 20\n",
        "n_classes = 3\n",
        "\n",
        "# Random features and labels\n",
        "X = torch.randn(n_samples, n_features)\n",
        "y = torch.randint(0, n_classes, (n_samples,))\n",
        "\n",
        "# Split into train/test (80/20)\n",
        "n_train = int(0.8 * n_samples)\n",
        "X_train, X_test = X[:n_train], X[n_train:]\n",
        "y_train, y_test = y[:n_train], y[n_train:]\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"Features: {n_features}\")\n",
        "print(f\"Classes: {n_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "E2-kvXj-T1tK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3708b061-b245-4593-e9cf-38aacbf4f834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Loss: 1.0995 | Train Acc: 35.12% | Test Acc: 37.00%\n",
            "Epoch 2/10 | Loss: 1.0927 | Train Acc: 38.75% | Test Acc: 36.50%\n",
            "Epoch 3/10 | Loss: 1.0874 | Train Acc: 41.12% | Test Acc: 39.50%\n",
            "Epoch 4/10 | Loss: 1.0835 | Train Acc: 42.50% | Test Acc: 38.00%\n",
            "Epoch 5/10 | Loss: 1.0788 | Train Acc: 43.12% | Test Acc: 34.50%\n",
            "Epoch 6/10 | Loss: 1.0741 | Train Acc: 43.88% | Test Acc: 34.00%\n",
            "Epoch 7/10 | Loss: 1.0689 | Train Acc: 45.25% | Test Acc: 35.50%\n",
            "Epoch 8/10 | Loss: 1.0634 | Train Acc: 45.88% | Test Acc: 34.50%\n",
            "Epoch 9/10 | Loss: 1.0579 | Train Acc: 47.62% | Test Acc: 35.50%\n",
            "Epoch 10/10 | Loss: 1.0517 | Train Acc: 47.75% | Test Acc: 35.00%\n"
          ]
        }
      ],
      "source": [
        "# Create datasets and loaders\n",
        "train_dataset = ToyDataset(X_train, y_train)\n",
        "test_dataset = ToyDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Create model\n",
        "torch.manual_seed(42)\n",
        "model = NeuralNetwork(num_inputs=n_features, num_outputs=n_classes)\n",
        "model.to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for features, labels in train_loader:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "        logits = model(features)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Evaluate every epoch\n",
        "    train_acc = compute_accuracy_gpu(model, train_loader, device)\n",
        "    test_acc = compute_accuracy_gpu(model, test_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "          f\"Loss: {avg_loss:.4f} | \"\n",
        "          f\"Train Acc: {train_acc*100:.2f}% | \"\n",
        "          f\"Test Acc: {test_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Escz5r-NT1tK"
      },
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "Congratulations! You've learned the essential PyTorch concepts:\n",
        "\n",
        "### âœ… Key Takeaways:\n",
        "\n",
        "1. **Tensors**: Multi-dimensional arrays (similar to NumPy) with GPU support\n",
        "2. **Autograd**: Automatic gradient computation for backpropagation\n",
        "3. **nn.Module**: Base class for building neural networks\n",
        "4. **DataLoader**: Efficient batching and data loading\n",
        "5. **Training Loop**:\n",
        "   - Forward pass â†’ Compute loss\n",
        "   - Backward pass â†’ Compute gradients\n",
        "   - Optimizer step â†’ Update weights\n",
        "6. **GPU Support**: Simple `.to(device)` for acceleration\n",
        "\n",
        "### ðŸŽ¯ Next Steps:\n",
        "\n",
        "- Experiment with different architectures\n",
        "- Try different optimizers (Adam, AdamW, etc.)\n",
        "- Explore real datasets (MNIST, CIFAR-10)\n",
        "- Learn about CNNs, RNNs, Transformers\n",
        "- Build your own projects!\n",
        "\n",
        "### ðŸ“š Further Resources:\n",
        "\n",
        "- **Official PyTorch Tutorials**: https://pytorch.org/tutorials/\n",
        "- **PyTorch Documentation**: https://pytorch.org/docs/\n",
        "- **Original Tutorial**: https://sebastianraschka.com/teaching/pytorch-1h/\n",
        "- **Books**:\n",
        "  - *Deep Learning with PyTorch* by Stevens, Antiga, and Viehmann\n",
        "  - *Machine Learning with PyTorch and Scikit-Learn* by Raschka et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6wLwm1hT1tK"
      },
      "source": [
        "---\n",
        "\n",
        "## Bonus Exercise: Build a Complete Project\n",
        "\n",
        "**Challenge**: Create a neural network to classify the Iris dataset\n",
        "\n",
        "**Steps**:\n",
        "1. Load the Iris dataset (sklearn)\n",
        "2. Create PyTorch datasets and dataloaders\n",
        "3. Build a neural network\n",
        "4. Train for multiple epochs\n",
        "5. Evaluate and report accuracy\n",
        "\n",
        "**Starter code below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "C2p4nN-rT1tK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee61e25-26f9-424b-98e0-dd0f04ce7e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 120 training, 30 test samples\n",
            "Features: 4, Classes: 3\n",
            "Epoch 1/190 | Loss: 1.1044 | Train Acc: 32.50% | Test Acc: 36.67%\n",
            "Epoch 2/190 | Loss: 1.0923 | Train Acc: 32.50% | Test Acc: 36.67%\n",
            "Epoch 3/190 | Loss: 1.0833 | Train Acc: 32.50% | Test Acc: 36.67%\n",
            "Epoch 4/190 | Loss: 1.0851 | Train Acc: 32.50% | Test Acc: 36.67%\n",
            "Epoch 5/190 | Loss: 1.0818 | Train Acc: 32.50% | Test Acc: 36.67%\n",
            "Epoch 6/190 | Loss: 1.0781 | Train Acc: 32.50% | Test Acc: 36.67%\n",
            "Epoch 7/190 | Loss: 1.0727 | Train Acc: 32.50% | Test Acc: 36.67%\n",
            "Epoch 8/190 | Loss: 1.0658 | Train Acc: 32.50% | Test Acc: 36.67%\n",
            "Epoch 9/190 | Loss: 1.0591 | Train Acc: 32.50% | Test Acc: 36.67%\n",
            "Epoch 10/190 | Loss: 1.0558 | Train Acc: 34.17% | Test Acc: 43.33%\n",
            "Epoch 11/190 | Loss: 1.0387 | Train Acc: 35.00% | Test Acc: 43.33%\n",
            "Epoch 12/190 | Loss: 1.0382 | Train Acc: 37.50% | Test Acc: 46.67%\n",
            "Epoch 13/190 | Loss: 1.0293 | Train Acc: 38.33% | Test Acc: 46.67%\n",
            "Epoch 14/190 | Loss: 1.0281 | Train Acc: 40.83% | Test Acc: 50.00%\n",
            "Epoch 15/190 | Loss: 1.0331 | Train Acc: 42.50% | Test Acc: 56.67%\n",
            "Epoch 16/190 | Loss: 1.0221 | Train Acc: 44.17% | Test Acc: 60.00%\n",
            "Epoch 17/190 | Loss: 1.0145 | Train Acc: 46.67% | Test Acc: 60.00%\n",
            "Epoch 18/190 | Loss: 1.0070 | Train Acc: 50.00% | Test Acc: 63.33%\n",
            "Epoch 19/190 | Loss: 1.0130 | Train Acc: 52.50% | Test Acc: 63.33%\n",
            "Epoch 20/190 | Loss: 1.0055 | Train Acc: 57.50% | Test Acc: 63.33%\n",
            "Epoch 21/190 | Loss: 0.9997 | Train Acc: 60.00% | Test Acc: 63.33%\n",
            "Epoch 22/190 | Loss: 0.9872 | Train Acc: 60.83% | Test Acc: 70.00%\n",
            "Epoch 23/190 | Loss: 0.9838 | Train Acc: 61.67% | Test Acc: 70.00%\n",
            "Epoch 24/190 | Loss: 0.9836 | Train Acc: 63.33% | Test Acc: 70.00%\n",
            "Epoch 25/190 | Loss: 0.9809 | Train Acc: 65.00% | Test Acc: 70.00%\n",
            "Epoch 26/190 | Loss: 0.9753 | Train Acc: 65.83% | Test Acc: 70.00%\n",
            "Epoch 27/190 | Loss: 0.9615 | Train Acc: 66.67% | Test Acc: 70.00%\n",
            "Epoch 28/190 | Loss: 0.9580 | Train Acc: 68.33% | Test Acc: 70.00%\n",
            "Epoch 29/190 | Loss: 0.9518 | Train Acc: 68.33% | Test Acc: 70.00%\n",
            "Epoch 30/190 | Loss: 0.9463 | Train Acc: 70.00% | Test Acc: 70.00%\n",
            "Epoch 31/190 | Loss: 0.9429 | Train Acc: 70.83% | Test Acc: 70.00%\n",
            "Epoch 32/190 | Loss: 0.9372 | Train Acc: 70.83% | Test Acc: 70.00%\n",
            "Epoch 33/190 | Loss: 0.9263 | Train Acc: 70.83% | Test Acc: 70.00%\n",
            "Epoch 34/190 | Loss: 0.9268 | Train Acc: 70.83% | Test Acc: 70.00%\n",
            "Epoch 35/190 | Loss: 0.9181 | Train Acc: 71.67% | Test Acc: 73.33%\n",
            "Epoch 36/190 | Loss: 0.9101 | Train Acc: 71.67% | Test Acc: 73.33%\n",
            "Epoch 37/190 | Loss: 0.9111 | Train Acc: 71.67% | Test Acc: 73.33%\n",
            "Epoch 38/190 | Loss: 0.8997 | Train Acc: 72.50% | Test Acc: 73.33%\n",
            "Epoch 39/190 | Loss: 0.8858 | Train Acc: 72.50% | Test Acc: 73.33%\n",
            "Epoch 40/190 | Loss: 0.8915 | Train Acc: 72.50% | Test Acc: 73.33%\n",
            "Epoch 41/190 | Loss: 0.8727 | Train Acc: 72.50% | Test Acc: 73.33%\n",
            "Epoch 42/190 | Loss: 0.8758 | Train Acc: 73.33% | Test Acc: 76.67%\n",
            "Epoch 43/190 | Loss: 0.8628 | Train Acc: 74.17% | Test Acc: 76.67%\n",
            "Epoch 44/190 | Loss: 0.8454 | Train Acc: 74.17% | Test Acc: 76.67%\n",
            "Epoch 45/190 | Loss: 0.8546 | Train Acc: 74.17% | Test Acc: 76.67%\n",
            "Epoch 46/190 | Loss: 0.8459 | Train Acc: 74.17% | Test Acc: 80.00%\n",
            "Epoch 47/190 | Loss: 0.8360 | Train Acc: 74.17% | Test Acc: 80.00%\n",
            "Epoch 48/190 | Loss: 0.8282 | Train Acc: 75.00% | Test Acc: 80.00%\n",
            "Epoch 49/190 | Loss: 0.8149 | Train Acc: 75.00% | Test Acc: 80.00%\n",
            "Epoch 50/190 | Loss: 0.8238 | Train Acc: 75.83% | Test Acc: 80.00%\n",
            "Epoch 51/190 | Loss: 0.8140 | Train Acc: 76.67% | Test Acc: 80.00%\n",
            "Epoch 52/190 | Loss: 0.7967 | Train Acc: 76.67% | Test Acc: 80.00%\n",
            "Epoch 53/190 | Loss: 0.8005 | Train Acc: 77.50% | Test Acc: 80.00%\n",
            "Epoch 54/190 | Loss: 0.7861 | Train Acc: 77.50% | Test Acc: 80.00%\n",
            "Epoch 55/190 | Loss: 0.7688 | Train Acc: 77.50% | Test Acc: 80.00%\n",
            "Epoch 56/190 | Loss: 0.7669 | Train Acc: 77.50% | Test Acc: 80.00%\n",
            "Epoch 57/190 | Loss: 0.7534 | Train Acc: 77.50% | Test Acc: 80.00%\n",
            "Epoch 58/190 | Loss: 0.7530 | Train Acc: 77.50% | Test Acc: 80.00%\n",
            "Epoch 59/190 | Loss: 0.7573 | Train Acc: 80.83% | Test Acc: 80.00%\n",
            "Epoch 60/190 | Loss: 0.7333 | Train Acc: 80.83% | Test Acc: 80.00%\n",
            "Epoch 61/190 | Loss: 0.7431 | Train Acc: 80.83% | Test Acc: 80.00%\n",
            "Epoch 62/190 | Loss: 0.7213 | Train Acc: 81.67% | Test Acc: 80.00%\n",
            "Epoch 63/190 | Loss: 0.7179 | Train Acc: 81.67% | Test Acc: 80.00%\n",
            "Epoch 64/190 | Loss: 0.7030 | Train Acc: 82.50% | Test Acc: 80.00%\n",
            "Epoch 65/190 | Loss: 0.7035 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 66/190 | Loss: 0.6893 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 67/190 | Loss: 0.6808 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 68/190 | Loss: 0.6787 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 69/190 | Loss: 0.6666 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 70/190 | Loss: 0.6629 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 71/190 | Loss: 0.6369 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 72/190 | Loss: 0.6594 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 73/190 | Loss: 0.6322 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 74/190 | Loss: 0.6257 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 75/190 | Loss: 0.6165 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 76/190 | Loss: 0.6120 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 77/190 | Loss: 0.6094 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 78/190 | Loss: 0.5823 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 79/190 | Loss: 0.6043 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 80/190 | Loss: 0.5819 | Train Acc: 83.33% | Test Acc: 80.00%\n",
            "Epoch 81/190 | Loss: 0.5874 | Train Acc: 84.17% | Test Acc: 83.33%\n",
            "Epoch 82/190 | Loss: 0.5683 | Train Acc: 84.17% | Test Acc: 83.33%\n",
            "Epoch 83/190 | Loss: 0.5475 | Train Acc: 84.17% | Test Acc: 83.33%\n",
            "Epoch 84/190 | Loss: 0.5598 | Train Acc: 84.17% | Test Acc: 83.33%\n",
            "Epoch 85/190 | Loss: 0.5414 | Train Acc: 84.17% | Test Acc: 86.67%\n",
            "Epoch 86/190 | Loss: 0.5557 | Train Acc: 84.17% | Test Acc: 86.67%\n",
            "Epoch 87/190 | Loss: 0.5496 | Train Acc: 84.17% | Test Acc: 86.67%\n",
            "Epoch 88/190 | Loss: 0.5240 | Train Acc: 84.17% | Test Acc: 86.67%\n",
            "Epoch 89/190 | Loss: 0.5142 | Train Acc: 84.17% | Test Acc: 86.67%\n",
            "Epoch 90/190 | Loss: 0.5150 | Train Acc: 84.17% | Test Acc: 86.67%\n",
            "Epoch 91/190 | Loss: 0.5214 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 92/190 | Loss: 0.5234 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 93/190 | Loss: 0.5051 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 94/190 | Loss: 0.5086 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 95/190 | Loss: 0.4938 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 96/190 | Loss: 0.4999 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 97/190 | Loss: 0.4845 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 98/190 | Loss: 0.4861 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 99/190 | Loss: 0.4777 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 100/190 | Loss: 0.4787 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 101/190 | Loss: 0.4566 | Train Acc: 85.00% | Test Acc: 90.00%\n",
            "Epoch 102/190 | Loss: 0.4561 | Train Acc: 85.83% | Test Acc: 90.00%\n",
            "Epoch 103/190 | Loss: 0.4583 | Train Acc: 85.83% | Test Acc: 90.00%\n",
            "Epoch 104/190 | Loss: 0.4494 | Train Acc: 85.83% | Test Acc: 90.00%\n",
            "Epoch 105/190 | Loss: 0.4606 | Train Acc: 85.83% | Test Acc: 90.00%\n",
            "Epoch 106/190 | Loss: 0.4551 | Train Acc: 85.83% | Test Acc: 90.00%\n",
            "Epoch 107/190 | Loss: 0.4449 | Train Acc: 85.83% | Test Acc: 90.00%\n",
            "Epoch 108/190 | Loss: 0.4317 | Train Acc: 85.83% | Test Acc: 90.00%\n",
            "Epoch 109/190 | Loss: 0.4326 | Train Acc: 85.83% | Test Acc: 90.00%\n",
            "Epoch 110/190 | Loss: 0.4280 | Train Acc: 85.83% | Test Acc: 90.00%\n",
            "Epoch 111/190 | Loss: 0.4221 | Train Acc: 85.83% | Test Acc: 90.00%\n",
            "Epoch 112/190 | Loss: 0.4142 | Train Acc: 86.67% | Test Acc: 90.00%\n",
            "Epoch 113/190 | Loss: 0.4171 | Train Acc: 86.67% | Test Acc: 90.00%\n",
            "Epoch 114/190 | Loss: 0.4231 | Train Acc: 86.67% | Test Acc: 90.00%\n",
            "Epoch 115/190 | Loss: 0.4212 | Train Acc: 87.50% | Test Acc: 90.00%\n",
            "Epoch 116/190 | Loss: 0.4063 | Train Acc: 87.50% | Test Acc: 90.00%\n",
            "Epoch 117/190 | Loss: 0.4150 | Train Acc: 87.50% | Test Acc: 90.00%\n",
            "Epoch 118/190 | Loss: 0.4014 | Train Acc: 88.33% | Test Acc: 90.00%\n",
            "Epoch 119/190 | Loss: 0.4144 | Train Acc: 90.00% | Test Acc: 90.00%\n",
            "Epoch 120/190 | Loss: 0.4102 | Train Acc: 90.00% | Test Acc: 90.00%\n",
            "Epoch 121/190 | Loss: 0.4045 | Train Acc: 90.00% | Test Acc: 90.00%\n",
            "Epoch 122/190 | Loss: 0.3980 | Train Acc: 90.83% | Test Acc: 90.00%\n",
            "Epoch 123/190 | Loss: 0.3990 | Train Acc: 90.83% | Test Acc: 90.00%\n",
            "Epoch 124/190 | Loss: 0.3820 | Train Acc: 91.67% | Test Acc: 90.00%\n",
            "Epoch 125/190 | Loss: 0.3951 | Train Acc: 90.83% | Test Acc: 90.00%\n",
            "Epoch 126/190 | Loss: 0.3817 | Train Acc: 90.83% | Test Acc: 90.00%\n",
            "Epoch 127/190 | Loss: 0.3919 | Train Acc: 91.67% | Test Acc: 93.33%\n",
            "Epoch 128/190 | Loss: 0.3683 | Train Acc: 91.67% | Test Acc: 93.33%\n",
            "Epoch 129/190 | Loss: 0.3790 | Train Acc: 90.83% | Test Acc: 93.33%\n",
            "Epoch 130/190 | Loss: 0.3783 | Train Acc: 91.67% | Test Acc: 93.33%\n",
            "Epoch 131/190 | Loss: 0.3778 | Train Acc: 92.50% | Test Acc: 93.33%\n",
            "Epoch 132/190 | Loss: 0.3812 | Train Acc: 92.50% | Test Acc: 93.33%\n",
            "Epoch 133/190 | Loss: 0.3573 | Train Acc: 92.50% | Test Acc: 93.33%\n",
            "Epoch 134/190 | Loss: 0.3421 | Train Acc: 91.67% | Test Acc: 93.33%\n",
            "Epoch 135/190 | Loss: 0.3451 | Train Acc: 91.67% | Test Acc: 93.33%\n",
            "Epoch 136/190 | Loss: 0.3536 | Train Acc: 92.50% | Test Acc: 93.33%\n",
            "Epoch 137/190 | Loss: 0.3434 | Train Acc: 91.67% | Test Acc: 93.33%\n",
            "Epoch 138/190 | Loss: 0.3452 | Train Acc: 91.67% | Test Acc: 93.33%\n",
            "Epoch 139/190 | Loss: 0.3432 | Train Acc: 92.50% | Test Acc: 93.33%\n",
            "Epoch 140/190 | Loss: 0.3440 | Train Acc: 93.33% | Test Acc: 93.33%\n",
            "Epoch 141/190 | Loss: 0.3381 | Train Acc: 92.50% | Test Acc: 96.67%\n",
            "Epoch 142/190 | Loss: 0.3337 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 143/190 | Loss: 0.3320 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 144/190 | Loss: 0.3282 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 145/190 | Loss: 0.3449 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 146/190 | Loss: 0.3331 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 147/190 | Loss: 0.3361 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 148/190 | Loss: 0.3198 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 149/190 | Loss: 0.3235 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 150/190 | Loss: 0.3282 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 151/190 | Loss: 0.3296 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 152/190 | Loss: 0.3254 | Train Acc: 93.33% | Test Acc: 96.67%\n",
            "Epoch 153/190 | Loss: 0.3345 | Train Acc: 95.00% | Test Acc: 96.67%\n",
            "Epoch 154/190 | Loss: 0.3101 | Train Acc: 95.00% | Test Acc: 96.67%\n",
            "Epoch 155/190 | Loss: 0.3072 | Train Acc: 95.00% | Test Acc: 96.67%\n",
            "Epoch 156/190 | Loss: 0.3199 | Train Acc: 95.00% | Test Acc: 96.67%\n",
            "Epoch 157/190 | Loss: 0.3128 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 158/190 | Loss: 0.3236 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 159/190 | Loss: 0.3020 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 160/190 | Loss: 0.3096 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 161/190 | Loss: 0.2969 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 162/190 | Loss: 0.2932 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 163/190 | Loss: 0.2955 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 164/190 | Loss: 0.3000 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 165/190 | Loss: 0.2895 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 166/190 | Loss: 0.3113 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 167/190 | Loss: 0.2890 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 168/190 | Loss: 0.2847 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 169/190 | Loss: 0.2754 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 170/190 | Loss: 0.2816 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 171/190 | Loss: 0.2705 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 172/190 | Loss: 0.2795 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 173/190 | Loss: 0.2705 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 174/190 | Loss: 0.2703 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 175/190 | Loss: 0.2774 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 176/190 | Loss: 0.2657 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 177/190 | Loss: 0.2612 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 178/190 | Loss: 0.2701 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 179/190 | Loss: 0.2803 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 180/190 | Loss: 0.2832 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 181/190 | Loss: 0.2699 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 182/190 | Loss: 0.2631 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 183/190 | Loss: 0.2643 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 184/190 | Loss: 0.2633 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 185/190 | Loss: 0.2694 | Train Acc: 95.83% | Test Acc: 96.67%\n",
            "Epoch 186/190 | Loss: 0.2808 | Train Acc: 96.67% | Test Acc: 100.00%\n",
            "Epoch 187/190 | Loss: 0.2437 | Train Acc: 96.67% | Test Acc: 100.00%\n",
            "Epoch 188/190 | Loss: 0.2592 | Train Acc: 96.67% | Test Acc: 100.00%\n",
            "Epoch 189/190 | Loss: 0.2476 | Train Acc: 96.67% | Test Acc: 100.00%\n",
            "Epoch 190/190 | Loss: 0.2535 | Train Acc: 96.67% | Test Acc: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Bonus challenge - Iris classification\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split and scale\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "print(f\"Dataset: {len(X_train)} training, {len(X_test)} test samples\")\n",
        "print(f\"Features: {X_train.shape[1]}, Classes: {len(iris.target_names)}\")\n",
        "\n",
        "# TODO: Complete the rest of the implementation\n",
        "# 1. Create datasets and dataloaders\n",
        "train_data = ToyDataset(X_train, y_train)\n",
        "test_data = ToyDataset(X_test, y_test)\n",
        "\n",
        "train_load = DataLoader(train_data, batch_size=35, shuffle=True)\n",
        "test_load = DataLoader(test_data, batch_size=35, shuffle=False)\n",
        "\n",
        "# 2. Define a neural network\n",
        "num_features = X_train.shape[1]\n",
        "num_classes = len(iris.target_names)\n",
        "\n",
        "torch.manual_seed(50)\n",
        "model_iris = NeuralNetwork(num_inputs=num_features, num_outputs=num_classes)\n",
        "model_iris.to(device)\n",
        "\n",
        "# 3. Train the model\n",
        "\n",
        "# Define optimizer\n",
        "optimizer_iris = torch.optim.SGD(model_iris.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 190\n",
        "for epoch in range(num_epochs):\n",
        "    model_iris.train()\n",
        "    total_loss_iris = 0\n",
        "\n",
        "    for features_iris, labels_iris in train_load:\n",
        "        features_iris, labels_iris = features_iris.to(device), labels_iris.to(device)\n",
        "\n",
        "        logits = model_iris(features_iris)\n",
        "        loss = F.cross_entropy(logits, labels_iris)\n",
        "\n",
        "        optimizer_iris.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_iris.step()\n",
        "\n",
        "        total_loss_iris += loss.item()\n",
        "\n",
        "    avg_loss = total_loss_iris / len(train_load)\n",
        "\n",
        "    # 4. Evaluate accuracy\n",
        "    train_acc = compute_accuracy_gpu(model_iris, train_load, device)\n",
        "    test_acc = compute_accuracy_gpu(model_iris, test_load, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
        "          f\"Loss: {avg_loss:.4f} | \"\n",
        "          f\"Train Acc: {train_acc*100:.2f}% | \"\n",
        "          f\"Test Acc: {test_acc*100:.2f}%\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}